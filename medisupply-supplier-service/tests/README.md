# ğŸ§ª Estructura de Tests - MediSupply Supplier Service

## ğŸ“ OrganizaciÃ³n de Tests

Los tests estÃ¡n organizados por tipo para facilitar el mantenimiento y la ejecuciÃ³n selectiva:

```
tests/
â”œâ”€â”€ unit/                    # Tests unitarios
â”‚   â”œâ”€â”€ test_carga_masiva.py
â”‚   â”œâ”€â”€ test_product.py
â”‚   â”œâ”€â”€ test_service.py
â”‚   â”œâ”€â”€ test_schemas.py
â”‚   â”œâ”€â”€ test_vendedor.py
â”‚   â””â”€â”€ test_plan.py
â”œâ”€â”€ integration/             # Tests de integraciÃ³n
â”‚   â””â”€â”€ test_integracion_carga_masiva.py
â”œâ”€â”€ e2e/                    # Tests end-to-end
â”‚   â””â”€â”€ test_smoke.py
â””â”€â”€ conftest.py            # ConfiguraciÃ³n compartida
```

## ğŸ¯ Tipos de Tests

### **Unit Tests** (`tests/unit/`)
- **PropÃ³sito**: Probar componentes individuales de forma aislada
- **CaracterÃ­sticas**: 
  - RÃ¡pidos de ejecutar
  - No dependen de servicios externos
  - Usan mocks/stubs cuando es necesario
  - Base de datos en memoria
- **Ejemplos**: Validaciones, servicios individuales, esquemas

### **Integration Tests** (`tests/integration/`)
- **PropÃ³sito**: Probar la integraciÃ³n entre mÃºltiples componentes
- **CaracterÃ­sticas**:
  - Usan TestClient de FastAPI
  - Prueban flujos completos HTTP
  - Verifican persistencia en base de datos
  - Incluyen validaciones de negocio
- **Ejemplos**: Carga masiva completa, flujos de API

### **End-to-End Tests** (`tests/e2e/`)
- **PropÃ³sito**: Probar flujos completos del sistema
- **CaracterÃ­sticas**:
  - Tests de humo (smoke tests)
  - Verifican que el sistema funciona end-to-end
  - Pueden usar servicios externos
- **Ejemplos**: Health checks, flujos crÃ­ticos

## ğŸš€ Comandos de EjecuciÃ³n

### **Scripts de Conveniencia**
```bash
# Tests unitarios
./run-unit-tests.sh

# Tests de integraciÃ³n
./run-integration-tests.sh

# Tests end-to-end
./run-e2e-tests.sh

# Todos los tests
./run-all-tests.sh
```

### **Comandos Pytest Directos**
```bash
# Por tipo de test
python -m pytest tests/unit/ -v -m "unit"
python -m pytest tests/integration/ -v -m "integration"
python -m pytest tests/e2e/ -v -m "e2e"

# Por marcador especÃ­fico
python -m pytest -v -m "unit"
python -m pytest -v -m "integration"
python -m pytest -v -m "smoke"

# Con cobertura
python -m pytest tests/ --cov=app --cov-report=html

# Tests especÃ­ficos
python -m pytest tests/unit/test_carga_masiva.py -v
python -m pytest tests/integration/test_integracion_carga_masiva.py -v
```

## ğŸ“Š Marcadores de Tests

Los tests estÃ¡n marcados para facilitar la ejecuciÃ³n selectiva:

- `@pytest.mark.unit`: Tests unitarios
- `@pytest.mark.integration`: Tests de integraciÃ³n  
- `@pytest.mark.e2e`: Tests end-to-end
- `@pytest.mark.smoke`: Tests de humo
- `@pytest.mark.slow`: Tests que tardan mÃ¡s tiempo

## ğŸ”§ ConfiguraciÃ³n

### **pytest.ini**
- ConfiguraciÃ³n centralizada de pytest
- Marcadores definidos
- Opciones de cobertura
- ConfiguraciÃ³n de reportes

### **conftest.py**
- Fixtures compartidas entre tests
- ConfiguraciÃ³n de base de datos de prueba
- Setup/teardown comÃºn

## ğŸ“ˆ Cobertura de Tests

El proyecto mantiene una cobertura mÃ­nima del 80%:
- Reportes en terminal y HTML
- Cobertura por mÃ³dulo
- IdentificaciÃ³n de lÃ­neas no cubiertas

## ğŸ¯ Mejores PrÃ¡cticas

### **Naming Convention**
- Archivos: `test_*.py`
- Clases: `Test*`
- Funciones: `test_*`

### **Estructura de Tests**
```python
def test_descripcion_del_comportamiento():
    # Arrange - Preparar datos
    # Act - Ejecutar acciÃ³n
    # Assert - Verificar resultado
```

### **Fixtures**
- Usar fixtures para setup comÃºn
- Fixtures con scope apropiado (function, class, module)
- Fixtures parametrizadas cuando sea necesario

### **Aislamiento**
- Cada test debe ser independiente
- Limpiar estado entre tests
- Usar base de datos en memoria para tests unitarios

## ğŸš¨ Troubleshooting

### **Tests que fallan**
1. Verificar que las dependencias estÃ©n instaladas
2. Revisar la configuraciÃ³n de base de datos
3. Verificar que los marcadores estÃ©n correctos

### **Problemas de cobertura**
1. Ejecutar con `--cov=app` para ver cobertura detallada
2. Revisar reporte HTML en `htmlcov/index.html`
3. Identificar mÃ³dulos con baja cobertura

### **Tests lentos**
1. Marcar tests lentos con `@pytest.mark.slow`
2. Ejecutar solo tests rÃ¡pidos: `pytest -m "not slow"`
3. Optimizar fixtures y setup
